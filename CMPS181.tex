% LaTeX template adapted from: https://www.overleaf.com/latex/templates/simple-math-homework-template/tbszsswsndrz
\documentclass[landscape,8pt]{extarticle}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{extsizes}
\usepackage[]{amsthm} %lets us use \begin{proof}
\usepackage[]{amssymb} %gives us the character \varnothing
\usepackage{amsmath} %for equations
\usepackage[]{listings} %for code blocks
\usepackage{graphicx} %for diagrams
\usepackage{fancyhdr} %for headers
\usepackage[letterpaper, margin=0.25in, headheight=10pt, headsep=0pt]{geometry}
\usepackage{tikz} % for drawings
\usepackage{multicol}
\usepackage{ifthen}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{color}
\usepackage{adjustbox}

\usepackage{wrapfig}
\usepackage{booktabs}
\setlist{nolistsep}
\setlist[itemize]{leftmargin=0.25pc,itemsep=0em}
\usetikzlibrary{arrows.meta,shapes.arrows,chains,decorations.pathreplacing}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
            {-1ex plus -.5ex minus -.2ex}%
            {0.5ex plus .2ex}%x
            {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
            {-1explus -.5ex minus -.2ex}%
            {0.5ex plus .2ex}%
            {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
            {-1ex plus -.5ex minus -.2ex}%
            {1ex plus .2ex}%
            {\normalfont\small\bfseries}}
\makeatother
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstdefinelanguage{JavaScript}{
  keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]"
}


\lstset{frame=single,
  language=c,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=fixed,
  basicstyle={\small\ttfamily},
  numbers=none,
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\newcommand{\code}{\lstinline}
\graphicspath{}
\pagestyle{empty}
\ifthenelse{\lengthtest { \paperwidth = 11in}}
{ \geometry{top=.3in,left=.25in,right=.25in,bottom=.25in} }
{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
}
\setlength{\parindent}{0em}
\setlength{\parskip}{-0.25em}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\rhead{Pete Wilcox | CruzID: pcwilcox | Student ID: 1593715}

\begin{document}
\footnotesize
\begin{multicols}{4}
    \setlength{\premulticols}{1pt}
    \setlength{\postmulticols}{1pt}
    \setlength{\multicolsep}{1pt}
    \setlength{\columnsep}{2pt}
    \begin{itemize}
        \item \code{ACID}
              \begin{itemize}
                  \item \code{ATOMICITY}: An atomic transaction happens as one unit, either the whole thing
                        commits or none of it does.
                  \item \code{CONSISTENCY}: A consistent transaction brings the DB from one valid state to
                        another valid state with respect to any constraints.
                  \item \code{ISOLATION}: Concurrent isolated transactions would have the same result if run sequentially.
                  \item \code{DURABILITY}: A committed transaction will remain committed even in the event of
                        a hardware failure.
              \end{itemize}
        \item \code{RAID Levels}
              \begin{itemize}
                  \item Level 0: No redundancy (just stripin)
                  \item Level 1: Mirrored (two identical copies)
                        \begin{itemize}
                            \item Each disk has an exact mirror image
                            \item Parallel reads; writes involve two disks
                            \item Maximum transfer rate = transfer rate of one disk
                        \end{itemize}
                  \item Level 0+1 (Level 10): Striping and Mirroring
                        \begin{itemize}
                            \item Parallel reads; writes involve two disks
                            \item Maximum transfer rate = aggregate bandwidth
                        \end{itemize}
                  \item Level 3: Bit-interleaved parity
                        \begin{itemize}
                            \item Striping Unit: one bit (or byte) (one check disk)
                            \item Each read and write request involves all disks; disk array can process one request at a time
                        \end{itemize}
                  \item Level 4: Block-interleaved parity
                        \begin{itemize}
                            \item Striping unit: one disk block (one check disk)
                            \item Parallel reads possible for small requests, large requests can utilize full bandwidth
                            \item Writes involve modified block \emph{and} check disk
                        \end{itemize}
                  \item Level 5: Block-interleaved distributed parity
                        \begin{itemize}
                            \item Similar to RAID level 4 but parity blocks are distributed over all disks
                        \end{itemize}
              \end{itemize}
        \item \code{Buffer Management in a DBMS}
              \begin{itemize}
                  \item DBMS maintains buffer pool of frames, each frame holds a page, info is in \code{<frame#, pageid>} table
                  \item Choice of frame replacement dictated by replacement policy such as LRU
                  \item When a page is requested:
                        \begin{itemize}
                            \item If requested page is not in pool:
                                  \begin{itemize}
                                      \item Choose a frame for replacement
                                      \item If that frame is dirty, write it to disk
                                      \item Read requested page into chosen frame
                                  \end{itemize}
                            \item Pin the page and return its address
                            \item When done the requestor must indicate whether the page has been modified (dirty bit) and unpin
                            \item Page in pool may be requested many times
                                  \begin{itemize}
                                      \item A pin count is used and a page is a candidate for replacement iff \code{pin_count = 0}
                                      \item Pinning increments pin count and unpinning decrements
                                  \end{itemize}
                            \item Concurrency control and recovery may entail additional I/O when a frame is chosen for replacement (write-ahead log protocol)
                            \item Frame is chosen for replacement using LRU, clock, MRU, etc
                            \item Sequential flooding: Caused by using LRU when the number of buffer frames is less than the number of pages in the file
                        \end{itemize}
              \end{itemize}
        \item \code{Files of Records}
              \begin{itemize}
                  \item Page or block is ok when doing I/O but higher levels of DBMS operate on \emph{records} and thus want \emph{files of records}
                  \item \code{FILE:} A collection of pages each containing a collection of records. Must support
                        \begin{itemize}
                            \item Insert (append)/delete/modify record
                            \item Read a particular record specified using \emph{record id}
                            \item Scan all records possibly with some conditions on the records to be retrieved
                        \end{itemize}
                  \item \code{Unordered ``Heap'' Files:}
                        \begin{itemize}
                            \item Simplest file structure that contains records in no particular (logical) order
                            \item As file grows and shrinks, disk pages are allocated and de-allocated
                            \item To support record-level operations we must:
                                  \begin{itemize}
                                      \item Keep track of the \emph{pages} in a file: \code{page id (pid)}
                                      \item Keep track of the \emph{free space} on a page
                                      \item Keep track of the \emph{records} on a page: \code{record id (rid)}
                                      \item Keep track of \emph{fields} within records
                                  \end{itemize}
                            \item Operations: create/destroy file, insert/delete record, fetch record with specific \code{rid}, scan all records
                        \end{itemize}
                  \item Record formats: \code{Fixed Length}
                        \begin{itemize}
                            \item Information about field types is the same for all records in file; it is stored in \emph{system catalogs}
                            \item Finding the $i^{th}$ field of a record does not require scanning the record
                        \end{itemize}
                  \item Record formats: \code{Variable length}
                        \begin{itemize}
                            \item Several alternative formats (\# of fields is fixed)
                            \item Fields delimited by special symbols (e.g. \$ between fields)
                            \item Fields preceded by lengths
                        \end{itemize}
                  \item  Record formats: \code{Variable length with directory}
                        \begin{itemize}
                            \item Use array of offsets at start of record
                        \end{itemize}
                  \item \code{Heap file implemented as a list}
                        \begin{itemize}
                            \item The header page id and heap file name must be stored someplace
                            \item Each page contains two extra pointers in this case
                            \item Refinement: use several lists for different degrees of free space
                        \end{itemize}
                  \item Page formats:
                        \begin{itemize}
                            \item File $\rightarrow$ collection of pages
                            \item Page -> collection of tuples/records
                            \item Query operators deal with tuples
                            \item Slotted page format:
                                  \begin{itemize}
                                      \item Each page has a collection of \emph{slots}
                                      \item Each slot contains a record
                                  \end{itemize}
                            \item RID: \code{<page id, slot number>}
                        \end{itemize}
                  \item \code{Heap file using a page directory}
                        \begin{itemize}
                            \item Page entries can include the number of free bytes on each page
                            \item Directory is a collection of pages; linked list is one possible implementation
                        \end{itemize}
                  \item \code{System catalogs:}
                        \begin{itemize}
                            \item For each relation:
                                  \begin{itemize}
                                      \item name, file, file structure
                                      \item name, type, length (if fixed) for each attribute
                                      \item Index name, target, and kind for each index
                                      \item also integrity constraints, defaults, nullability, etc
                                  \end{itemize}
                            \item For each index: structure (e.g. B+ tree) and search key fields
                            \item For each view: view name and definition (including query)
                            \item Plus statistics, authorization, buffer pool size, etc
                        \end{itemize}
                  \item \code{Column Stores:}
                        \begin{itemize}
                            \item Store data ``vertically''
                            \item Contrast with a ``row-store'' that stores all the attributes of a tuple/record contiguously
                            \item Each column can be stored as a separate file and compressed
                            \item SAP HANA:
                                  \begin{itemize}
                                      \item Dictionary compression per column
                                      \item Column main: read-optimized store for immutable data. Uses high data compression and heuristic algoriths to order data to maximize secondary compression
                                      \item Column delta: write-optimized store for inserts, updates, deletes. Uses less compression, appends updates to the end, and merges with main periodically.
                                  \end{itemize}
                            \item Additional types: prefix coding, run length coding, cluster coding, sparse coding, indirect coding
                        \end{itemize}
              \end{itemize}
        \item \code{Indexes:}
              \begin{itemize}
                  \item Speeds up selections on the search key fields for the index
                  \item Contains a collection of data entries and supports efficient retrieval of all data entires $k^*$ with a given key value $k$
              \end{itemize}
        \item \code{B+ Tree Indexes}
              \begin{itemize}
                  \item Leaf pages contain \emph{data entries} and are chained (prev \& next)
                  \item Non-leaf pages have \emph{index entries}, used to direct searches
                  \item Insert/delete at $\log_F N$, keep tree \emph{height-balanced} ($F = $ fanout, $N = $ \# leaf pages)
                  \item Minimum 50\% occupancy (in all nodes except root). Each node contains $d \leq m \leq 2d$ entries; $d = $ the \emph{order} of the tree.
                  \item Typical order $d = 100$
                  \item Percentage of node that is full is more useful, typical fill-factor 67\%
                  \item Average \emph{fanout} for non-leaves $F = 133$
                  \item Inserting a data entry:
                        \begin{itemize}
                            \item Find correct leaf $L$
                            \item Put data entry onto $L$
                            \item If $L$ has enough space, done
                            \item Otherwise, must split $L$. Redistribute entries evenly, copy up the middle key (key must still exist in leaf). Insert index entry pointing to $L_2$ into parent of $L$.
                            \item This can happen recursively: if parent of $L$ grows, need to push up middle key.
                            \item Splits ``grow'' the tree; root split increases height.
                        \end{itemize}
                  \item Deleting a data entry:
                        \begin{itemize}
                            \item Start at root, find leaf $L$ where entry belongs
                            \item Remove the entry
                            \item If $L$ is at least half full, done
                            \item Otherwise, if $L$ has only $d-1$ entries, try to redistribute, borrowing from sibling (adjacent node with same parent)
                            \item If redistribution fails, merge $L$ and sibling
                            \item If merge occurred, must delete entry from parent (pointing to merged node)
                            \item Merge can propagate to root, decreasing height of the tree
                        \end{itemize}
              \end{itemize}
        \item \code{Hash-Based Indexes:}
              \begin{itemize}
                  \item Good for equality selections
                  \item Index is a collection of \emph{buckets}. Each bucket = \emph{primary page} plus zero or more \emph{overflow pages} (called \emph{static} hashing). Buckets contain data entries.
                  \item \emph{Hashing function} $h$: $h(r) = $ bucket in which (data entry for) record $r$ belongs. $h$ looks at the \emph{search key} fields of $r$.
              \end{itemize}
        \item Alternatives for Data Entry $k^*$ in index:
              \begin{itemize}
                  \item In a data entry $k^*$ we can store: an actual data record, or \code{<k, RID>}, or \code{<k, list of RIDs>}
                  \item Choice of alternative for entries is orthogonal to the indexing technique
              \end{itemize}
        \item Alternative 1: data records live in index
              \begin{itemize}
                  \item Index structure is actually a file organization for the data records
                  \item At most one index on a given collection of data can use this Alternative
                  \item If data records are very large, \# of leaf pages containing data entries is high.
              \end{itemize}
        \item Alternatives 2 and 3: Key/RID or Key/RIDlist:
              \begin{itemize}
                  \item Data entries are typically much smaller than data records
                  \item Alternative 3 is more compact but leads to variable-sized data entries, even if the search keys are of fixed length
              \end{itemize}
        \item Index classification:
              \begin{itemize}
                  \item \emph{Primary vs Secondary:} if search key contains the primary key, index is called the primary index
                  \item \emph{Clustered vs Unclustered:} If order of data records is the same as (or close to) the order of stored data records then index is called a clustered index.
              \end{itemize}
        \item A back of the envelope cost model:
              \begin{itemize}
                  \item $B$: the number of data pages
                  \item $R$: number of records per page
                  \item $D$: average time to read or write a disk page
                  \item $F$: average fanout for a non-leaf page
              \end{itemize}
        \item Indexes with composite search keys:
              \begin{itemize}
                  \item Composite search keys: search on a combination of fields
                  \item Equality query: every field value is equal to a constant value
                  \item Range query: some field value doesn't have equality test
                  \item Data entries in index sorted by search key to support range queries
              \end{itemize}
        \item \code{ISAM:} Index-Sequential Access Method
              \begin{itemize}
                  \item Index file has first key on each page, can binary search index then scan the page.
                  \item \emph{Static} structure, inserts and deletes only affect leaf or overflow pages.
                  \item If index is very large, recursively create a second layer (and so on).
                  \item \emph{File Creation:} Leaf pages first allocated sequentially, sorted by search key; then index pages allocated, and then overflow pages.
                  \item \emph{Index entries:} \code{<key value, page id>}; they `direct' searches for \emph{data entries} which are in leaf pages
                  \item \emph{Search:} Start at root; use key comparisons to go to leaf. I/O cost $\propto \log_F N$ where $F = $ \# entries/index pg, $N = $ \# leaf pgs
                  \item \emph{Insert:} Find leaf where data entry belongs and put it there, using overflow page if necessary.
                  \item \emph{Delete:} Finda nd remove from leaf; if empty overflow page, deallocate
              \end{itemize}
        \item \code{B-Tree Prefix Key Compression:} Increase fan-out by reducing the size of search keys on interior nodes. key values only direct traffic so we only need the minimum length for that
        \item \code{Bulk Loading of a B+ Tree}
              \begin{itemize}
                  \item Creating a new B+ tree by inserting one at a time is very slow, bulk loading is better
                  \item \emph{Initialization:} Sort all data entries, insert pointer to first (leaf) page in a new (root) page
                  \item Index entries for leaf pages always entered into right-most index page just above leaf level. When this fills up it splits.
              \end{itemize}
        \item \code{Log-Structured Merge Tree:} Sequential trees of exponentially larger size. Inserts go to smallest smallest tree, deletes insert tombstone records, spill to next-deeper level on overflow
        \item \code{R-Tree:} Tree of rectangles, search for intersections between them
        \item \code{Static Hash-based Indexes:}
              \begin{itemize}
                  \item \# primary pages is fixed, allocated sequentially, never de-allocated; overflow pages if needed
                  \item $h(k) \mod M = $ bucket(page) to which data entry with key $k$ belongs ($M = $ \# buckets)
                  \item Buckets contain data entries
                  \item Hash function works on \emph{search key field} of record $r$. Must distribute over range $0 \dots M-1$
                  \item $h(key) = (a * key + b)$ usually works well; $a$ and $b$ are constants to tune $h$
                  \item \emph{Long overflow chains} can develop and degrade performance.
              \end{itemize}
        \item \code{Extendible Hashing:}
              \begin{itemize}
                  \item Situation: bucket (primary page) becomes full. Solved by doubling number of buckets instead of using an overflow page
                  \item Use directory wich pointers to buckets. Double the number of buckets by doubling the directory and splitting buckets as needed
                  \item Only one bucket at a time splits. No overflow pages
                  \item \emph{Global Depth} is the last $d$ bits after hashing and indexes into the directory to determine which bucket is used
                  \item \emph{Local Depth} is used for each bucket. If the $LD == GD$ and the bucket splits, the directory must double.
              \end{itemize}
        \item \code{Bitmap Indexes:}
              \begin{itemize}
                  \item Index which allows for fast equality checks. Order the records in some $O(1)$ way and maintain one or more bit vectors storing their values for particular fields.
                  \item One bitmap for each distinct domain value, and one bitmap for \code{NULL} if the column can be null.
                  \item Can use \code{XOR} operation to reduce number of maps needed by one.
              \end{itemize}
        \item \code{External Sorting:}
        \begin{itemize}
            \item Goal: Need to sort more data than will fit in memory, efficiently.
            \item \code{2-Way Sort:} Requires 3 buffers
            \begin{itemize}
                \item Pass 0: read a page, sort it, write it out (only one buffer page used)
                \item Pass 1, 2, 3, ...: Read and merge pairs of runs. (Three buffer pages are used)
            \end{itemize}
            \item Sorting $N=2^k Pages of Data$:
            \begin{itemize}
                \item Pass 0: read, sort, write $\rightarrow 2^k$ 1-page runs
                \item Pass 1: Read + merge 1-page pairs, write $\rightarrow 2^{k-1}$ 2-page runs
                \item Pass 2: Read + merge 2-page pairs, write $\rightarrow 2^{k-2}$ 4-page runs
                \item Pass $k-1$: Read + merge $2^{k-2}$-page pairs, write $\rightarrow 2 2^{k-1}$-page runs
                \item Pass $k$: Read + merge $2^{k-1}$-page pairs, write $\rightarrow 1 2^k$-page result
            \end{itemize}
            \item 2-Way External merge sort: $N$ pages in file $\implies \lceil \log_2 N \rceil + 1$ passes, total I/O cost $= 2 N \left ( \lceil \log_2 N \rceil + 1 \right )$
            \item \emph{General} external merge sort:
            \begin{itemize}
                \item Sorting a file with $N$ pages using $B$ buffer pages.
                \item Pass 0: use $B$ buffer pages. Produce $\lceil N / B \rceil$ sorted runs of $B$ pages each.
                \item Pass 2, ... etc: merge $b - 1$ runs
                \item Number of passes: $1 + \left \lceil \log_{B-1} \lceil N / B \rceil \right \rceil $
                \item Cost $= 2N * ($\# of passes $)$
            \end{itemize}
            \item \emph{Double Buffering}
            \begin{itemize}
                \item To reduce wait time for I/O request to complete, can \emph{prefetch} into \code{shadow block}
                \item Potentially more passes; in practice, most files still sorted in 2-3 passes.
            \end{itemize}
            \item \emph{B+ Tree as ``Sorted Access Path''}
            \begin{itemize}
                \item Scenario: table to be retrieved in some order has a B+ tree index on the ordering columns
                \item Idea: retrieve records in order by traversing the B+ tree's leaf pages
                \item Very good idea if the tree is clustered, otherwise probably a bad idea
            \end{itemize}
        \end{itemize}
        \item \code{Query Processing}
        \begin{itemize}
            \item \emph{Access Paths:}
            \begin{itemize}
                \item An access path is a method of retrieving tuples: file scan or index that matches a selection in the query
                \item A tree index \emph{matches} (a conjunction of) terms that involve only attributes in a \emph{prefix} of the search key.
                \item A hash index \emph{matches} (a conjunction of) terms that has a term \emph{attribute = value} for every attribute in the search key of the index.
            \end{itemize}
            \item Selection conditions often first converted to be in CNF (\code{AND}ing of \code{OR}s)
            \item One approach to selections:
            \begin{itemize}
                \item Find the \emph{most selective access path}, retrieve tuples using it, then apply any remaining terms which don't match the index
                \item Most selective access path: an index or file scan that we estimate will require the fewest page I/Os
                \item Terms that match this index reduce the number of tuples retrieved; other terms used to filter the retrieved tuples on the fly, but don't prevent retrieval of the tuples/pages
            \end{itemize}
            \item Using an index for selections: cost depends on \# qualifying tuples and clustering
            \begin{itemize}
                \item Cost of finding qualifying data entries (typically small) plus cost of retrieving the actual records themselves (can be large without clustering)
            \end{itemize}
            \item \emph{Duplicate Elimination}
            \begin{itemize}
                \item Relational algebra projection removes duplicates: SQL systems don't remove duplicates unless the keyword \code{DISTINCT} is specified
                \item Sorting approach: sort on \code{<sid, bid>} and remove duplicates. Can optimize by dropping unwanted columns while sorting.
                \item Hashing approach: hash on \code{<sid, bid>} to create \emph{partitions}. Load partitions into memory one at a time, build an in-memory hash structure, and eliminate duplicates within it.
            \end{itemize}
            \item Notation:
            \begin{itemize}
                \item Pages in a heap relation $R$: Pages$_R$
                \item Tuples per page for a relation $R$: TPP$_R$
                \item Number of tuples in $R$: Card$_R$
                \item Card$_R = $ Pages$_R * $ TPP$_R$
            \end{itemize}
            \item \emph{Simple Nested Loops Join}: foreach tuple in $R$, for each tuple in $S$, if $r_i == s_j$ then add \code{<r,s>} to result.
            \begin{itemize}
                \item For each tuple in the \emph{outer} relation $R$ we scan the entire \emph{inner} relation $S$. Cost: Pages$_R$ + Card$_R * $ Pages$_S$
                \item Page-oriented nested loops join: for each \emph{page} of $R$ get each \emph{page} of $S$ and write out matching pairs of tuples
            \end{itemize}
            \item \emph{Index Nested Loops}
            \begin{itemize}
                \item If there is an index on the join column of one relation, can make it the inner and exploit the index. Cost: Pages$_R$ + Card$_R$ * cost of finding matching $S$ tuples
                \item For each $R$ tuple, cost of probing $S$ index is about 1.2 for hash index, 2-4 for B+ tree. Cost of then finding $S$ tuples (assuming alt. 2 or 3) depends on clustering. Clustered typically 1 I/O, unclustered up to 1 I/O per matching $S$ tuple.
            \end{itemize}
            \item \emph{Block Nexted Loops Join}
            \begin{itemize}
                \item Use one page as an input buffer for scanning the inner $S$, one pas as the output buffer, and use \emph{all} remaining pages to hold ``block'' of pages of outer $R$.
                \item For each block of $R$, hash each data entry to a hash table. Then compare all entries in $S$.
            \end{itemize}
            \item \emph{Join: Sort-Merge } $\left (R \bowtie_{i=j} S \right)$
            \begin{itemize}
                \item Sort $R$ and $S$ on the join column, then scan them to do a `merge' (on join column) and then output result tupls.
                \item $R$ is scanned once; each $S$ group is scanned once per matching $R$ tuple.
            \end{itemize}
            \item \emph{Statistics and Catalogs}
            \begin{itemize}
                \item Catalogs typically contain at least: \emph{\# tuples} and \emph{\# pages} in each relation; \emph{\# distinct key values} and \emph{\# pages} for each index; \emph{index height}, \emph{low/high key values} for each tree index
                \item Catalogs updated periodically: updating each time data changes is too expensive and approximation is fine
            \end{itemize}
            \item \emph{Cost estimation:} For each plan must estimate cost
            \begin{itemize}
                \item Estimate \emph{Cost} of each operation in plan-tree: depends on input cardinalities
                \item Estimate \emph{Size} of result for each operation in the tree; for seelctions and joins assume independence of predicates
            \end{itemize}
            \item \emph{Size estimation and reduction factors}
            \begin{itemize}
                \item Maximum \# of tuples in result is the product of cardinalities in the \code{FROM} clause
                \item Reduction Factor $RF$ associated with each \emph{term} reflects the impact of \emph{term} in reducing result size
            \end{itemize}
            \item \emph{Grace Hash-Join}
            \begin{itemize}
                \item Like a two-phase index nested loop join
                \item \emph{Build phase:} Partition both relations using hash function $h$: $R$ tuples in partition $i$ will only match $S$ tuples in partition $i$.
                \item \emph{Match phase:} Read in a partition of $R$, hash it using $h_2 (<>h)$. Scan matching partition of $S$ searching for its $R$ matches.
                \item \# partitions $k \leq B-1$ and Pages$_R /k$ (size of largest partition to be held in memory) $< B-1$.
                \item Assuming uniformy-sized partitions and maximizing $k$: $k = B-1$ and Pages$_R /k < B-1$ so $(B-1)^2 > $Pages$_R \implies B > \sqrt{\text{Pages}_R}$
                \item Can hash-join \emph{recursively} to reduce the amount of memory needed
                \item In build phase: Read + write both relations: $2*($Pages$_R + $Pages$_S)$
                \item In match phase: Read both relations; Pages$_R + $Pages$_S$ I/Os
            \end{itemize}
            \item Sort-Merge Join vs Hash Join:
            \begin{itemize}
                \item Given a a reasonable amount of memory $(B > \sqrt{\text{Pages}_R})$, both have cost $3*(\text{Pages}_R + \text{Pages}_s)$ I/Os
                \item Hash join is superior is relation sizes differ greatly because it needs less memory, and is also highly parallelizable
                \item Sort-Merge is less sensitive to data skew and its result is sorted
            \end{itemize}
            \item \emph{More general join conditions:}
            \begin{itemize}
                \item Equalities over several attributes: for INL, build index on composite key. For sort-merge and hash join, sort/hash-partition on the combination of the join columns
                \item Inequality conditions: for INL, need (clustered!) B+ tree index; hash-join not usable; merge-join possible; block NL the best
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \begin{center}
        \begin{tabular}{ | c | c | } \hline
            Var     & Desc                                      \\ \hline
            $B$     & the number of data pages                  \\ \hline
            $R$     & number of records per page                \\ \hline
            $D$     & average time to read or write a disk page \\ \hline
            $F$     & average fanout for a non-leaf page        \\ \hline
        \end{tabular}
    \end{center}
    \begin{center}
        \begin{adjustbox}{angle=270}
            \begin{tabular}{ | c | c | c | c | c | c | } \hline
                ()            & Scan          & Equality              & Range                                    & Insert        & Delete        \\ \hline
                Heap          & $BD$          & $0.5 BD$              & $BD$                                     & $2D$          & Search + $D$  \\ \hline
                Sorted        & $BD$          & $D \log_2 B$          & $D(\log_2 B +$ \# matching pages$)$      & Search + $BD$ & Search + $BD$ \\ \hline
                Clustered     & $1.5 BD$      & $D \log_F 1.5 B$      & $D(\log_F 1.5B + $ \# matching pages$)$  & Search + $D$  & Search + $D$  \\ \hline
                Unclust. Tree & $BD(R+0.15)$  & $D(1 + \log_F 0.15B)$ & $D(\log_F 0.15B + $ \# matching pages$)$ & Search + $2D$ & Search + $2D$ \\ \hline
                Unclust. Hash & $BD(R+0.125)$ & $2D$                  & $BD$                                     & Search + $2D$ & Search + $2D$ \\ \hline
            \end{tabular}
        \end{adjustbox}
    \end{center}
\end{multicols}

\end{document}
