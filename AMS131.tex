% LaTeX template adapted from: https://www.overleaf.com/latex/templates/simple-math-homework-template/tbszsswsndrz
\documentclass[landscape,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[]{amsthm} %lets us use \begin{proof}
\usepackage[]{amssymb} %gives us the character \varnothing
\usepackage{amsmath} %for equations
\usepackage[]{listings} %for code blocks
\usepackage{graphicx} %for diagrams
\usepackage{fancyhdr} %for headers
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{tikz} % for drawings
\usepackage{multicol}
\usepackage{ifthen}
\usetikzlibrary{arrows.meta,shapes.arrows,chains,decorations.pathreplacing}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

\graphicspath{}
\pagestyle{empty}
\ifthenelse{\lengthtest { \paperwidth = 11in}}
	{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}
\setlength{\parindent}{0em}
\setlength{\parskip}{0em plus 0.5ex}

\begin{document}
\raggedright
\footnotesize
\begin{multicols}{3}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

    \subsection*{Experiments and Events:}
        \subparagraph*{Def:} 
            An experiment is a process whose outcome is not known in advance with certainy.

        \subparagraph*{Sample Space: } 
            Collection of \textit{all} possible outcomes of an experiment. $S$ or $\Omega$. Each outcome is an element of the sample space $s \in S$.

    \subsection*{Operations: }
        \subparagraph*{Union: } 
            \[ \begin{matrix}
                x \in S: A \cup B&= \{x \in A \text{ or } x \in B\}    & \\
                A \cup B = B \cup A & A \cup A = A \\ 
                A \cup \emptyset = A & A \cup S = S \\
                A \subset B \Rightarrow A \cup B = B & \\
                A_1, A_2, \dots, A_n \Rightarrow A_1 \cup A_2 \cup \dots \cup A_n = \bigcup\limits_{i=1}^{i=n} A_i & \\
                \bigcup\limits_{i=1}^{\infty} A_i \rightarrow \bigcup\limits_{i\in I}A_i 
                (A \cup B) \cup C = A \cup (B \cup C) = A \cup B \cup C 
            \end{matrix} \]
            

        \subparagraph*{Intersections: } 
            \( A \cap B = \{ x \in A \text{ and } x \in B\} = AB \)            
            
            \begin{itemize}
                \item[] \(A \cap B = B \cap A \)
                \item[] \(A \cap A = A \)
                \item[] \(A \cap \emptyset = \emptyset \)
                \item[] \(A \cap S = A\)
                \item[] \(A \subset B \Rightarrow A \cap B\)
                \item[] \(\bigcap\limits_{i\in I}A_i = \bigcap\limits_{i=1}^{\infty}A_i\)
                \item[] \(\bigcap\limits_{i=1}^{n}A_i = A_1 \cap A_2 \cap \dots \cap A_n \)
                \item[] \( (A \cap B) \cap C = A \cap (B \cap C) = A \cap B \cap C\)
            \end{itemize}
        
        \subparagraph*{Complements: } 
            \(A^c = \{x \in S: x \notin A\}\)
            
            \begin{itemize}
                \item[] \((A^c)^c = A\)
                \item[] \(\emptyset^c = S\)
                \item[] \(S^c = \emptyset\)
                \item[] \(A\cup A^c = S\)
                \item[] \(A \cap A^c = \emptyset\)
            \end{itemize}
    
    \subsection*{Disjoint Events: } 
        $A$ and $B$ are \textit{disjoint} or \textit{mutually exclusive} if $A$ and $B$ have no outcomes in common. This happens only if \(A \cap B = \emptyset \).
        
        \subparagraph*{Def:} 
            A collection \(A_1, \dots, A_n\) is a collection of disjoint evens if and only if \(A_i \cap A_j = \emptyset, \forall i, j, i \neq j\)
            
            \begin{itemize}
                \item[] \(\left (\bigcup\limits_{i\in I} A_i \right)^c = \bigcap\limits_{i\in I}A_i^c \)
                \item[] \((A\cup B)^c = A^c \cap B^c \)
                \item[] \(x\in(A\cap B)^c 
                \Rightarrow x \notin A \text{ and } x\notin B 
                \Rightarrow x \in A^c \text{ and } x \in B^c 
                \Rightarrow x \in A^c \cap B^c \)
            \end{itemize}

    \subsection*{Probabilities: } 
        Functions over $S$ that measure the likelihood of events.

            \begin{itemize}
                \item[] $\forall A: Pr(A) \geq 0$
                \item[] $Pr(S) = 1$
                \item[] For every \textit{infinite sequence} of \textit{disjoint} events \( A_1, A_2, \dots (A_i \in S): 
                Pr\left(\bigcup\limits_{i=1}^{\infty}A_i\right) = \sum\limits_{i=1}^{\infty} Pr(A_i) 
                Pr(A_1 \cup A_2 \cup \dots \cup A_n \cup \dots) 
                = Pr(A_1) + Pr(A_2) + \dots + Pr(A_n) + \dots \)
                \item[]\(Pr(\emptyset) = 0\)
                \item[]\(Pr(\bigcup\limits_{i=1}^{n}A_i)=Pr(\bigcup\limits_{i=1}^{n}A_i + \bigcup\limits_{i=n+1}^{\infty}\emptyset) = \sum\limits_{i=1}^{n}Pr(A_i)\)
                \item[]\(Pr(A^c) = 1-Pr(A)\)
                \item[]\(A \subset B \Rightarrow Pr(A) \leq Pr(B)\)
                \item[]\(\forall A: 0 \leq Pr(A) \leq 1\)
                \item[]\(Pr(A\cup B) = Pr(A) + Pr(B) - Pr(A\cap B)\)
            \end{itemize}

    \subsection*{Finite Sample Spaces: } 
        \(S := \{s_1, s_2, \dots, s_n \}\)
        
        \subparagraph*{} 
            To obtain a probability distribution over $S$ we need to specify \(Pr(s_i) = P_i, \forall i = 1, 2, \dots, n\), such that \(\sum\limits_{i=1}^{n}P_i = 1\)

        \subparagraph*{Def: } 
            A sample space $S$ with $n$ outcomes $s_1, \ldots, s_n$ is a \textit{simple sample space} if the probability assigned to each outcome is $\frac{1}{n}$. If $A$ contains $m$ outcomes then \(Pr(A) = \frac{m}{n}\).

    \subsection*{Counting Methods: }
        
        \subparagraph*{Multiplication Rule: } 
            Suppose an experiment has $k$ parts \((k \geq 2)\) such that the $i^{th}$ part of the experiement has $n_i$ possible outcomes, $i = 1, \ldots, k$, and that \textit{all possible outcomes can occur regardless of which outcomes have occurred in other parts}. The sample space $S$ will contain vectors of the form $(u_1, u_2, \ldots, u_k)$. $u_i$ is one of the $n_i$ possible outcomes of part $i$. The total number of vectors is $n_1 \cdot n_2 \cdot \ldots \cdot n_k$.
        
        \subparagraph*{Permutations: } 
            Given an array of $n$ elements the first position can be filled with $n$ different elements, the second with $n-1$, and so on. $n \cdot (n-1) \cdot (n-2) \cdot \ldots \cdot 1 = n!$

            \begin{itemize}
                \item[] \(P_{n,k} = \frac{n!}{(n-k)!}\)
                \item[] \(P_{n,n} = n!\)
            \end{itemize}

        \subparagraph*{Combinations: }
            In general we can "combine" $n$ elements taking $k$ at a time in \(C_{n,k} = \frac{P_{n,k}}{k!} = \frac{n!}{(n-k)!k!} = {n \choose k} \). 

        \subparagraph*{Multinomial Coefficeints: } 
            Consider splitting $n$ elements into $k (k \geq 2)$ groups in a way such that group $j$ gets $n_j$ elements and $\sum\limits_{j = 1}^{k}n_j = n$. The $n_1$ elements in the first group can be selected in ${n \choose n_1}$, the second in ${n-n_1 \choose n_2}$, the third in ${n-n_1-n_2 \choose n_3}$ and so on until we complete the $k$ groups. Then: ${n \choose n_1}\cdot{n-n_1 \choose n_2}\cdot{n-n_1-n_2 \choose n_3}\cdot \ldots \cdot{n_k \choose n_k} = {n \choose n_1, n_2, \ldots, n_k}$
        \subparagraph*{Probability of union:}
            If $A_1, A_2, \ldots, A_n$ are \textit{disjoint events} then 
            \(Pr(A_1 \cup A_2 \cup \ldots \cup A_n) 
            = Pr(\bigcup\limits_{i=1}^{n}A_i) 
            = Pr(A_1) + Pr(A_2) + \ldots + Pr(A_n) 
            = \sum\limits_{i=1}^{n}Pr(A_i) \)
        \subparagraph*{} 
            If the events are not disjoint: 
            \( A_1, A_2: Pr(A_1 \cup A_2) = Pr(A_1) + Pr(A_2) - Pr(A_1 \cap A_2) 
            A_1, A_2, A_3: Pr(A_1) + Pr(A_2) + Pr(A_3) - Pr(A_1 \cap A_2) - Pr(A_1 \cap A_3) - Pr(A_2 \cap A_3) + Pr(A_1 \cap A_2 \cap A_3)\)     
        
    \subsection*{Conditional Probability: }
        \paragraph*{}
            If $A, B$ are events such that $Pr(A) > 0$ and $Pr(B) > 0$ then $Pr(B|A) = \frac{Pr(A\cap B)}{Pr(A)}$ and 
            $Pr(A|B) = \frac{Pr(A \cap B)}{Pr(B)}$ 
            Furthermore: $Pr(A \cap B) = Pr(B|A) \cdot Pr(A)$ and $Pr(A \cap B) = Pr(A|B) \cdot Pr(B)$. In general: $Pr(A_1 \cap A_2 \cap \ldots \cap A_n) = Pr(A_1) \cdot Pr(A_2 | A_1) \cdot \ldots \cdot Pr(A_n | A_1 \cap A_2 \cap \ldots \cap A_{n-1})$
        \paragraph*{Independence:}
            $A, B$ are independent events if $Pr(A|B) = Pr(A)$ and $Pr(B|A) = Pr(B)$. Then, if $A, B$ are independent: $Pr(A \cap B) = Pr(A|B) \cdot Pr(B) = Pr(A) \cdot Pr(B)$ and $Pr(A \cap B) = Pr(B | A) \cdot Pr(A) = Pr(B) \cdot Pr(A)$. In general if $A_1, A_2, \ldots, A_n$ are independent, $Pr(A_1 \cap A_2 \cap \ldots \cap A_n) = Pr(A_1) \cdot Pr(A_2) \cdot \ldots \cdot Pr(A_n)$. Note that if $A \cap B = \emptyset$ then the two events are \textit{not independent}. note that if $A, B$ are independent then $A, B^c$ are also independent.
        
        \paragraph*{Conditionally Independent: } 
            $A_1, \ldots, A_k$ are \textit{conditionally independent} given $B$ if, for every subset $A_{i_1}, \ldots, A_{i_m}: Pr(A_{i_1} \cap \ldots \cap A_{i_m} | B) = Pr(A_{i_1} | B) \cdot \ldots \cdot Pr(A_{i_m} | B)$.
        
        \paragraph*{Partitions: }
            Let $B_1, \ldots, B_k$ be such that $B_i \cap B_j = \emptyset \forall i \neq j$ and $\bigcup\limits_{i = 1}^{k} B_i = S$. Then these events form a partition of $S$. \\
            $A = A \cap S = A \cap \left(\bigcup\limits_{i = 1}^{k} B_i\right) = (A \cap B_1) \cup (A \cap B_2) \cup \ldots \cup (A \cap B_k)$. Then: $Pr(A) = Pr(A \cap S) = Pr(A \cap \left(\bigcup\limits_{i = 1}^{k} B_i \right)) = Pr(A \cap B_1) + Pr(A \cap B_2) + \ldots + Pr(A \cap B_k) = Pr(A|B_1) \cdot Pr(B_1) + Pr(A|B_2)\cdot Pr(B_2) + \ldots + Pr(A|B_k) \cdot Pr(B_k) = \sum\limits_{i=1}^{k} Pr(A|B_i)\cdot Pr(B_i)$. So, if $B_1, \ldots, B_k$ are a partition of $S$: $Pr(A) = \sum\limits_{i=1}^{k}Pr(A|B_i)\cdot Pr(B_i)$
        
        \paragraph*{Bayes' Theorem:}
            $B_1, \ldots, B_k :=$ a partition of $S$ such that $Pr(B_j) > 0, j = 1, \ldots, k$. Assume you have $A$ such that $Pr(A) > 0$. Then: $Pr(B_i|A) = \frac{Pr(A|B_i) \cdot Pr(B_i)}{Pr(A)} = \frac{Pr(A|B_i) \cdot Pr(B_i)}{\sum\limits_{j=1}^{k}Pr(A|B_j) \cdot Pr(B_j)}$
        
        \paragraph*{Random Variables:}
            \subparagraph*{Def:}
                A real-valued function on $S$ isa  random variable. A random variable $X$ is a functions that assigns a real number $X(s)=x$ to each possible outcome $s \in S$: $X:S \Rightarrow \mathcal{D}$. $x:=$ a realization of the random variable, $x \in \mathcal{D}$.
            \subparagraph*{Notation: } 
                We will be computing $Pr(X \in E)$ for $E \subset \mathcal{D} = Pr(s \in S: X(s) \in E)$
            \subparagraph*{Discrete Probability Distributions: }
                A r.v. $X$ has a discrete distribution if it takes a countable number of values. The probability function of a discrete r.v. is $f_X(x) = Pr(X=x)$. Properties:

                \begin{itemize}
                    \item[] $0 \leq f_X(x) \leq 1$
                    \item[] $\forall x \notin \mathcal{D}: f_X(x) = 0$
                    \item[] $\sum\limits_{x \in \mathcal{D}} f_X(x) = 1$
                    \item[] $Pr(X \in A) = \sum\limits_{x \in A} f_X (x)$
                \end{itemize}

            \subparagraph*{Uniform Distribution: }
                $X = x, x \in \{1, 2, \ldots, k\}$ with all values $x$ equally likely. The p.f. is $f_X(x) = Pr(X=x) = $\[ \begin{cases} \frac      {1}{k} & x = 1, 2, \ldots, k \\
                0 & o.w. \end{cases}
                    \]
            \subparagraph*{Bernoulli Distribution: }
                An event $A$ happens with probability $p$ \\
            $X$ = \[ \begin{cases} 1 & \text{if $A$ happens} \\ 0 & \text{if $A^c$ happens} \end{cases} \] \\
                $f_X(x) =$ \[ 
                    \begin{cases} 
                        (1-p)   & x = 0 \\ 
                        p       & x = 1 \\
                        0       & \text{o.w.} 
                    \end{cases} \]
            \subparagraph*{Binomial:}
                    $n$ Bernoulli trials repeated independently with probability of success $p$. $X :=$ number of success in $n$ trials. $x \in \{0, 1, \ldots, n\}$. $f_X(x) = Pr(X=x) =$
                    \[
                        \begin{cases}
                            {n \choose x} p^x (1-p)^{n-x}   &   x=0, \ldots, n \\
                            0                               &   \text{o.w.}
                        \end{cases}
                        \]
                        $X \sim Bin(n,p)$
            \subparagraph*{Hypergeometric: }
                    A box with $A$ red balls and $B$ blue balls. $n$ balls are drawn \textit{without replacement}. $X :=$ number of red balls. $X \leq min(n, A)$. $max(n-B, 0) \leq X \leq min(n,A)$. $f_X(x) = Pr(X=x) =$ \[
                        \begin{cases}
                            \frac{ {A \choose x} \cdot {B \choose n - x}}{{A + B \choose n}}     &   \text{for } max(n-B,0) \leq x \leq min(n,A) \\
                            0   &   \text{o.w.}
                        \end{cases}
                    \]
            
            \subparagraph*{Negative-Binomial: }
                    We repeat Bernoulli trials until $r$ successes are observed. $X :=$ number of failures $= \{0, 1, \ldots\}$. $p :=$ probability of success. $Pr(X = x) = Pr(x \text{ failures before $r$ successes}) = Pr(x \text{ failures and $r-1$ successes in $x + r - 1$ trials}) \cdot Pr(\text{one success in last trial}) = \left[{x+r-1 \choose x } (1-p)^x p^{r-1} \right] \cdot p = {x + r - 1 \choose x} (1 - p)^x p^r$. $f_X(x) = $ \[
                        \begin{cases}
                            {x+r-1 \choose x} (1-p)^x p^r   &   x=0,1,2,\ldots \\
                            0       & \text{o.w.}
                        \end{cases}
                    \]
            \subparagraph*{Geometric: }
                    Negative binomial with $r=1. f_X(x) = \begin{cases} (1-p)^x p & x=0,1,\ldots \\ 0 & \text{o.w.} \end{cases}$

            \subparagraph*{Poisson: }
                    Counts occurences of an event. $X$ is a Poisson r.v. with parameter $\lambda$ (intensity) if the p.f. is $f_X(x) = \begin{cases} \frac{e^{-\lambda}\lambda^x}{x!} & x=0,1,2,\ldots \\ 0 & \text{o.w.} \end{cases}$ with $\lambda>0$.
            \subparagraph*{Continuous Random Variables: }
                    A r.v. $X$ has a continuous distribution if there is a non-negative $f$ such that $Pr(a \leq X \leq b) = \int_{a}^{b} f(x)dx$. $f$ is the probability density function p.d.f. 
            \subparagraph*{Cumulative Distribution Function: } 
                    (c.d.f.) For any r.v. $X$ the c.d.f. is given by $F(x) = Pr(X \leq x)$. Properties: \begin{itemize}
                        \item[] $\forall x: 0 \leq F(x) \leq 1$
                        \item[] $F(x)$ is non-decreasing, i.e. if $x_1 < x_2 \Rightarrow \{X \leq x_1 \} \subset \{X \leq x_2 \}$ and so $Pr(X \leq x_1) \leq Pr(X \leq x_2) \Rightarrow F(x_1) \leq F(x_2)$
                        \item[] $\lim_{x \to -\infty} F(x) = 0$ and $\lim_{x \to \infty} F(x) = 1$
                    \end{itemize}
                    For a continuous r.v.:
                    \begin{itemize}
                        \item[] $F(x) = Pr(X \leq x) = \int_{-\infty}^{\infty}f(t)dt$
                        \item[] $F'(x) = f(x)$
                        \item[] $Pr(a < X \leq b) = Pr(a \leq X \leq b) = Pr(a \leq X < b) = Pr(a < X < b)$
                    \end{itemize}
                In general, $X \sim Unif[a,b] \Rightarrow f(x) = \begin{cases} \frac{1}{b-a} & a \leq x \leq b \\ 0 & \text{o.w.} \end{cases}$. The c.d.f.: $F(x) = \begin{cases} 0 & x < a \\ \frac{x-a}{b-a} & a \leq x \leq b \\ 1 x > b \end{cases}$
                    \subparagraph*{Quantile Function: }
                        $X$ continuous r.v. $F^{-1}(p)$ is the quantile function of $X$ for $0 \leq p \leq 1. F^{-1}(p) = x \Rightarrow p = F(x)$.

                    \subparagraph*{Joint Continuous Distributions: }
                        Joint p.d.f. given by $f_{X,Y}(x,y) = Pr((X,Y) \in A) = \iint\limits_{A} f(x,y)dxdy$.
                        
                    \subsection*{Other Stuff:}
                        \[ \sum\limits_{i=1}^{n}i = \frac{n(n+1)}{2},\sum\limits_{i=1}^{n}i^2 = \frac{n(n+1)(2n+1)}{6}\]
                        \[ \sum\limits_{i=0}^{n}c^i = \frac{c^{n+1}-1}{c-1}, c \neq 1; \sum\limits_{i=0}^{\infty}c^i = \frac{1}{1-c} \]
                \end{multicols}

\end{document}