% LaTeX template adapted from: https://www.overleaf.com/latex/templates/simple-math-homework-template/tbszsswsndrz
\documentclass[landscape,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[]{amsthm} %lets us use \begin{proof}
\usepackage[]{amssymb} %gives us the character \varnothing
\usepackage{amsmath} %for equations
\usepackage[]{listings} %for code blocks
\usepackage{graphicx} %for diagrams
\usepackage{fancyhdr} %for headers
\usepackage[letterpaper, margin=1in]{geometry}
\usepackage{tikz} % for drawings
\usepackage{multicol}
\usepackage{ifthen}
\usepackage{enumitem}
\setlist{nolistsep}
\setlist[itemize]{leftmargin=0.5pc,itemsep=0.25em}
\usetikzlibrary{arrows.meta,shapes.arrows,chains,decorations.pathreplacing}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
            {-1ex plus -.5ex minus -.2ex}%
            {0.5ex plus .2ex}%x
            {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
            {-1explus -.5ex minus -.2ex}%
            {0.5ex plus .2ex}%
            {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
            {-1ex plus -.5ex minus -.2ex}%
            {1ex plus .2ex}%
            {\normalfont\small\bfseries}}
\makeatother

\graphicspath{}
\pagestyle{empty}
\ifthenelse{\lengthtest { \paperwidth = 11in}}
{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
}
\setlength{\parindent}{0em}
\setlength{\parskip}{-0.25em}

\begin{document}
    \footnotesize
    \begin{multicols}{3}
    \setlength{\premulticols}{1pt}
    \setlength{\postmulticols}{1pt}
    \setlength{\multicolsep}{1pt}
    \setlength{\columnsep}{2pt}

    \subparagraph*{Union: } 
        \begin{itemize}[noitemsep]
            \item[] $x \in S: A \cup B= \{x \in A \text{ or } x \in B\}$
            \item[] $A \cup B = B \cup A$
            \item[] $A \cup A = A$
            \item[] $A \cup \emptyset = A$
            \item[] $A \cup S = S $
            \item[] $A \subset B \Rightarrow A \cup B = B$
            \item[] $A_1, A_2, \dots, A_n \Rightarrow A_1 \cup A_2 \cup \dots \cup A_n = \bigcup\limits_{i=1}^{i=n} A_i$
            \item[] $\bigcup\limits_{i=1}^{\infty} A_i \rightarrow \bigcup\limits_{i\in I}A_i$
            \item[] $(A \cup B) \cup C = A \cup (B \cup C) = A \cup B \cup C $
        \end{itemize}

    \subparagraph*{Intersections: }             
        \begin{itemize}[noitemsep]
            \item[] \( A \cap B = \{ x \in A \text{ and } x \in B\} = AB \)
            \item[] \(A \cap B = B \cap A \)
            \item[] \(A \cap A = A \)
            \item[] \(A \cap \emptyset = \emptyset \)
            \item[] \(A \cap S = A\)
            \item[] \(A \subset B \Rightarrow A \cap B\)
            \item[] \(\bigcap\limits_{i\in I}A_i = \bigcap\limits_{i=1}^{\infty}A_i\)
            \item[] \(\bigcap\limits_{i=1}^{n}A_i = A_1 \cap A_2 \cap \dots \cap A_n \)
            \item[] \( (A \cap B) \cap C = A \cap (B \cap C) = A \cap B \cap C\)
        \end{itemize}

    \subparagraph*{Complements: } 
        \begin{itemize}[noitemsep]
            \item[] \(A^c = \{x \in S: x \notin A\}\)
            \item[] \((A^c)^c = A\)
            \item[] \(\emptyset^c = S\)
            \item[] \(S^c = \emptyset\)
            \item[] \(A\cup A^c = S\)
            \item[] \(A \cap A^c = \emptyset\)
        \end{itemize}

    \subparagraph*{Disjoint Events: } 
        $A$ and $B$ are \textit{disjoint} or \textit{mutually exclusive} if $A$ and $B$ have no outcomes in common. This happens only if \(A \cap B = \emptyset \). A collection \(A_1, \dots, A_n\) is a collection of disjoint evens if and only if \(A_i \cap A_j = \emptyset, \forall i, j, i \neq j\)

        \begin{itemize}[noitemsep]
            \item[] \(\left (\bigcup\limits_{i\in I} A_i \right)^c = \bigcap\limits_{i\in I}A_i^c \)
            \item[] \((A\cup B)^c = A^c \cap B^c \)
            \item[] \(x\in(A\cap B)^c 
            \Rightarrow x \notin A\) and \(x\notin B \)
            \item[] \(\Rightarrow x \in A^c \) and \(x \in B^c \)
            \item[] \(\Rightarrow x \in A^c \cap B^c \)
        \end{itemize}

    \subparagraph*{Probabilities: } 

        \begin{itemize}[noitemsep]
            \item[] $\forall A: Pr(A) \geq 0$
            \item[] $Pr(S) = 1$
            \item[] $Pr(\emptyset) = 0$
            \item[] $Pr(A^c) = 1-Pr(A)$
            \item[] $A \subset B \Rightarrow Pr(A) \leq Pr(B)$
            \item[] $\forall A: 0 \leq Pr(A) \leq 1$
        \end{itemize}
        For every \textit{infinite sequence} of \textit{disjoint} events: \( A_1, A_2, \dots (A_i \in S):\)
        \begin{itemize}
            \item[] $Pr\left(\bigcup\limits_{i=1}^{\infty}A_i\right) = \sum\limits_{i=1}^{\infty} Pr(A_i)$
            \item[] $=  Pr(A_1 \cup A_2 \cup \dots \cup A_n \cup \dots)$
            \item[] $= Pr(A_1) + Pr(A_2) + \dots + Pr(A_n) + \dots$
            \item[] $Pr(\bigcup\limits_{i=1}^{n}A_i) = Pr(\bigcup\limits_{i=1}^{n}A_i + \bigcup\limits_{i=n+1}^{\infty}\emptyset) $
            \item[] $= \sum\limits_{i=1}^{n}Pr(A_i)$
            \item[] $Pr(A\cup B) = Pr(A) + Pr(B) - Pr(A\cap B)$
        \end{itemize}

    \subparagraph*{Finite Sample Spaces: } 
        \(S := \{s_1, s_2, \dots, s_n \}\) To obtain a probability distribution over $S$ we need to specify \(Pr(s_i) = P_i, \forall i = 1, 2, \dots, n\), such that \(\sum\limits_{i=1}^{n}P_i = 1\). A sample space $S$ with $n$ outcomes $s_1, \ldots, s_n$ is a \textit{simple sample space} if the probability assigned to each outcome is $\frac{1}{n}$. If $A$ contains $m$ outcomes then \(Pr(A) = \frac{m}{n}\).

    \subparagraph*{Multiplication Rule: } 
        Suppose an experiment has $k$ parts \((k \geq 2)\) such that the $i^{th}$ part of the experiement has $n_i$ possible outcomes, $i = 1, \ldots, k$, and that \textit{all possible outcomes can occur regardless of which outcomes have occurred in other parts}. The sample space $S$ will contain vectors of the form $(u_1, u_2, \ldots, u_k)$. $u_i$ is one of the $n_i$ possible outcomes of part $i$. The total number of vectors is $n_1 \cdot n_2 \cdot \ldots \cdot n_k$.

    \subparagraph*{Permutations: } 
        Given an array of $n$ elements the first position can be filled with $n$ different elements, the second with $n-1$, and so on. $n \cdot (n-1) \cdot (n-2) \cdot \ldots \cdot 1 = n!$

        \begin{itemize}
            \item[] \(P_{n,k} = \frac{n!}{(n-k)!}\)
            \item[] \(P_{n,n} = n!\)
        \end{itemize}

    \subparagraph*{Combinations: }
        In general we can "combine" $n$ elements taking $k$ at a time in \(C_{n,k} = \frac{P_{n,k}}{k!} = \frac{n!}{(n-k)!k!} = {n \choose k} \). 

    \subparagraph*{Multinomial Coefficients: } 
        Consider splitting $n$ elements into $k (k \geq 2)$ groups in a way such that group $j$ gets $n_j$ elements and $\sum_{j = 1}^{k}n_j = n$. The $n_1$ elements in the first group can be selected in ${n \choose n_1}$, the second in ${n-n_1 \choose n_2}$, the third in ${n-n_1-n_2 \choose n_3}$ and so on until we complete the $k$ groups. Then: ${n \choose n_1}\cdot{n-n_1 \choose n_2}\cdot{n-n_1-n_2 \choose n_3}\cdot \ldots \cdot{n_k \choose n_k} = {n \choose n_1, n_2, \ldots, n_k}$

    \subparagraph*{Probability of union:}
        If $A_1, A_2, \ldots, A_n$ are \textit{disjoint events} then 
        \begin{itemize}
            \item[] $Pr(A_1 \cup A_2 \cup \ldots \cup A_n) $
            \item[] $= Pr(\bigcup\limits_{i=1}^{n}A_i) $
            \item[] $= Pr(A_1) + Pr(A_2) + \ldots + Pr(A_n) $
            \item[] $= \sum\limits_{i=1}^{n}Pr(A_i) $
        \end{itemize}

    If the events are not disjoint: 
        \begin{itemize}
            \item[] Two events $A_1, A_2: Pr(A_1 \cup A_2) = Pr(A_1) + Pr(A_2) - Pr(A_1 \cap A_2)$
            \item[] Three events: $A_1, A_2, A_3: Pr(A_1 \cup A_2 \cup A_3)$
            \item[] $= Pr(A_1) + Pr(A_2) + Pr(A_3) - Pr(A_1 \cap A_2) - Pr(A_1 \cap A_3) - Pr(A_2 \cap A_3) + Pr(A_1 \cap A_2 \cap A_3)$
        \end{itemize}  

    \subparagraph*{Conditional Probability: }
        If $A, B$ are events such that $Pr(A) > 0$ and $Pr(B) > 0$ then 
        \begin{itemize}
            \item[] $Pr(B|A) = \frac{Pr(A\cap B)}{Pr(A)}$ and 
            \item[] $Pr(A|B) = \frac{Pr(A \cap B)}{Pr(B)}$ 
            \item[] Furthermore: 
            \item[] $Pr(A \cap B) = Pr(B|A) \cdot Pr(A)$ and 
            \item[] $Pr(A \cap B) = Pr(A|B) \cdot Pr(B)$. 
            \item[] In general: $Pr(A_1 \cap A_2 \cap \ldots \cap A_n) $
            \item[] $= Pr(A_1) \cdot Pr(A_2 | A_1) \cdot \ldots \cdot Pr(A_n | A_1 \cap A_2 \cap \ldots \cap A_{n-1})$
        \end{itemize}

    \subparagraph*{Independence:}
        $A, B$ are independent events if $Pr(A|B) = Pr(A)$ and $Pr(B|A) = Pr(B)$. Then, if $A, B$ are independent: 
        \begin{itemize}
            \item[] $Pr(A \cap B) = Pr(A|B) \cdot Pr(B) = Pr(A) \cdot Pr(B)$ and 
            \item[] $Pr(A \cap B) = Pr(B | A) \cdot Pr(A) = Pr(B) \cdot Pr(A)$.
            \item[] In general if $A_1, A_2, \ldots, A_n$ are independent: 
            \item[] $Pr(A_1 \cap A_2 \cap \ldots \cap A_n) = Pr(A_1) \cdot Pr(A_2) \cdot \ldots \cdot Pr(A_n)$. 
            \item[] Note that if $A \cap B = \emptyset$ then the two events are \textit{not independent}. 
            \item[] Note that if $A, B$ are independent then $A, B^c$ are also independent.
        \end{itemize}

    \subparagraph*{Conditionally Independent: } 
        $A_1, A_2, \ldots, A_k$ are \textit{conditionally independent} given $B$ if, for every subset $A_{i_1}, \ldots, A_{i_m}: Pr(A_{i_1} \cap \ldots \cap A_{i_m} | B) = Pr(A_{i_1} | B) \cdot \ldots \cdot Pr(A_{i_m} | B)$.

    \subparagraph*{Partitions: }
        Let $B_1, \ldots, B_k$ be such that $B_i \cap B_j = \emptyset \forall i \neq j$ and $\bigcup_{i = 1}^{k} B_i = S$. Then these events form a partition of $S$.
        \begin{itemize}
            \item[] $A = A \cap S = A \cap \left(\bigcup_{i = 1}^{k} B_i\right) $
            \item[] $= (A \cap B_1) \cup (A \cap B_2) \cup \ldots \cup (A \cap B_k)$. Then: 
            \item[] $Pr(A) = Pr(A \cap S) $
            \item[] $= Pr(A \cap \left(\bigcup_{i = 1}^{k} B_i \right)) $
            \item[] $= Pr(A \cap B_1) + Pr(A \cap B_2) + \ldots + Pr(A \cap B_k)$
            \item[] $=Pr(A|B_1) \cdot Pr(B_1) + Pr(A|B_2)\cdot Pr(B_2) + \ldots + Pr(A|B_k) \cdot Pr(B_k) $
            \item[] $= \sum_{i=1}^{k} Pr(A|B_i)\cdot Pr(B_i)$. 
            \item[]So, if $B_1, \ldots, B_k$ are a partition of $S$: $Pr(A) = \sum_{i=1}^{k}Pr(A|B_i)\cdot Pr(B_i)$
        \end{itemize}

    \subparagraph*{Bayes' Theorem:}
        $B_1, \ldots, B_k :=$ a partition of $S$ such that $Pr(B_j) > 0, j = 1, \ldots, k$. Assume you have $A$ such that $Pr(A) > 0$. Then: $$Pr(B_i|A) = \frac{Pr(A|B_i) \cdot Pr(B_i)}{Pr(A)} = \frac{Pr(A|B_i) \cdot Pr(B_i)}{\sum\limits_{j=1}^{k}Pr(A|B_j) \cdot Pr(B_j)}$$

    \subparagraph*{Random Variables:}
        A real-valued function on $S$ is a  random variable. A random variable $X$ is a functions that assigns a real number $X(s)=x$ to each possible outcome $s \in S$: $X:S \Rightarrow \mathcal{D}$. $x:=$ a realization of the random variable, $x \in \mathcal{D}$. We will be computing $Pr(X \in E)$ for $E \subset \mathcal{D} = Pr(s \in S: X(s) \in E)$
    
    \subparagraph*{Discrete Probability Distributions: }
    A r.v. $X$ has a discrete distribution if it takes a countable number of values. The probability function of a discrete r.v. is $f_X(x) = Pr(X=x)$. Properties:
    \begin{itemize}
        \item[] $0 \leq f_X(x) \leq 1$
        \item[] $\forall x \notin \mathcal{D}: f_X(x) = 0$
        \item[] $\sum\limits_{x \in \mathcal{D}} f_X(x) = 1$
        \item[] $Pr(X \in A) = \sum\limits_{x \in A} f_X (x)$
    \end{itemize}

    \subparagraph*{Uniform Distribution: }
        $X = x, x \in \{1, 2, \ldots, k\}$ with all values $x$ equally likely. The p.f. is $f_X(x) = Pr(X=x) =  \begin{cases} \frac      {1}{k} & x = 1, 2, \ldots, k \\ 0 & o.w. \end{cases} $

    \subparagraph*{Bernoulli Distribution: }
        An event $A$ happens with probability $p$:
        \begin{itemize}
            \item[] $X = \begin{cases} 1 & \text{if $A$ happens} \\ 0 & \text{if $A^c$ happens} \end{cases} $
            \item[] $f_X(x) =
    \begin{cases} 
    (1-p)   & x = 0 \\ 
    p       & x = 1 \\
    0       & \text{o.w.} 
    \end{cases} $
\end{itemize}

    \subparagraph*{Binomial:}
        $n$ Bernoulli trials repeated independently with probability of success $p$. 
        \begin{itemize}
            \item[] $X :=$ number of success in $n$ trials. 
            \item[] $x \in \{0, 1, \ldots, n\}$. 
            \item[] $f_X(x) = Pr(X=x) =
    \begin{cases}
    {n \choose x} p^x (1-p)^{n-x}   &   x=0, \ldots, n \\
    0                               &   \text{o.w.}
    \end{cases} $
            \item[] $X \sim Bin(n,p)$
        \end{itemize}

    \subparagraph*{Hypergeometric: }
        \begin{itemize}
            \item[] A box with $A$ red balls and $B$ blue balls.
            \item[] $n$ balls are drawn \textit{without replacement}. 
            \item[] $X :=$ number of red balls. 
            \item[] $X \leq min(n, A)$. 
            \item[] $max(n-B, 0) \leq X \leq min(n,A)$. 
            \item[] $f_X(x) =
    \begin{cases}
    \frac{ {A \choose x} \cdot {B \choose n - x}}{{A + B \choose n}}     &   \text{for } max(n-B,0) \leq x \leq min(n,A) \\
    0   &   \text{o.w.}
    \end{cases}
    $
\end{itemize}

    \subparagraph*{Negative-Binomial: }
        \begin{itemize}
            \item[] We repeat Bernoulli trials until $r$ successes are observed. 
            \item[] $X :=$ number of failures $= \{0, 1, \ldots\}$. 
            \item[] $p :=$ probability of success. 
            \item[] $Pr(X = x) = Pr(x \text{ failures before $r$ successes}) $
            \item[] $ = Pr(x \text{ failures and $r-1$ successes in $x + r - 1$ trials}) \cdot Pr(\text{one success in last trial}) $
            \item[] $= \left[{x+r-1 \choose x } (1-p)^x p^{r-1} \right] \cdot p = {x + r - 1 \choose x} (1 - p)^x p^r$. 
            \item[] $f_X(x) =
    \begin{cases}
    {x+r-1 \choose x} (1-p)^x p^r   &   x=0,1,2,\ldots \\
    0       & \text{o.w.}
    \end{cases}$
\end{itemize}

    \subparagraph*{Geometric: }
        Negative binomial with $r=1.$
        \begin{itemize}
            \item[] $f_X(x) = \begin{cases} (1-p)^x p & x=0,1,\ldots \\ 0 & \text{o.w.} \end{cases}$
        \end{itemize}

    \subparagraph*{Poisson: }
        Counts occurences of an event. $X$ is a Poisson r.v. with parameter $\lambda$ (intensity) if the p.f. is $f_X(x) = \begin{cases} \frac{e^{-\lambda}\lambda^x}{x!} & x=0,1,2,\ldots \\ 0 & \text{o.w.} \end{cases}$ with $\lambda>0$.

    \subparagraph*{Continuous Random Variables: }
        A r.v. $X$ has a continuous distribution if there is a non-negative $f$ such that $Pr(a \leq X \leq b) = \int_{a}^{b} f(x)dx$. $f$ is the probability density function p.d.f. To find the normalizing constant integrate the p.d.f. over the domain, it must equal 1.

    \subparagraph*{Cumulative Distribution Function: } 
        (c.d.f.) For any r.v. $X$ the c.d.f. is given by $F(x) = Pr(X \leq x)$. Properties: 
        \begin{itemize}
            \item[] $\forall x: 0 \leq F(x) \leq 1$
            \item[] $F(x)$ is non-decreasing, i.e. if $x_1 < x_2 \Rightarrow \{X \leq x_1 \} \subset \{X \leq x_2 \}$ and so $Pr(X \leq x_1) \leq Pr(X \leq x_2) \Rightarrow F(x_1) \leq F(x_2)$
            \item[] $\lim_{x \to -\infty} F(x) = 0$ and $\lim_{x \to \infty} F(x) = 1$
        \end{itemize}
        For a continuous r.v.:
        \begin{itemize}
            \item[] $F(x) = Pr(X \leq x) = \int_{-\infty}^{\infty}f(t)dt$
            \item[] $F'(x) = f(x)$
            \item[] $Pr(a < X \leq b) = Pr(a \leq X \leq b) = Pr(a \leq X < b) = Pr(a < X < b)$
        \end{itemize}
    In general, $X \sim Unif[a,b] \Rightarrow f(x) = \begin{cases} \frac{1}{b-a} & a \leq x \leq b \\ 0 & \text{o.w.} \end{cases}$. \\
    The c.d.f.: $F(x) = \begin{cases} 0 & x < a \\ \frac{x-a}{b-a} & a \leq x \leq b \\ 1 x > b \end{cases}$

    \subparagraph*{Quantile Function: }
        $X$ continuous r.v. $F^{-1}(p)$ is the quantile function of $X$ for $0 \leq p \leq 1. F^{-1}(p) = x \Rightarrow p = F(x)$.

    \subparagraph*{Joint Continuous Distributions: }
    Joint p.d.f. given by $f_{X,Y}(x,y) = Pr((X,Y) \in A) = \iint\limits_{A} f(x,y)dxdy$. To find the joint c.d.f. just integrate.

    \subparagraph*{Marginal Distributions: }
        In general for discrete r.v. $f_X(x) = \sum_{y}f(x,y)$ and $f_Y(y) = \sum_{x}f(x,y)$. In the case of 2 cont. r.v. $f_X(x) = \int_{-\infty}^{\infty}f(x,y)dy$ and $f_Y(y) = \int_{-\infty}^{\infty}f(x,y)dx$. $f_X(x)$ is the marginal p.d.f. of $X$. $f_Y(y)$ is the marginal p.d.f. of $Y$.

    \subparagraph*{Independence: }
        Two r.v. are independent if they produce independent events: $Pr(X \in A, Y \in B) = Pr(X \in A) \cdot Pr(Y \in B)$. This implies: $Pr(X \leq x, Y \leq y) = Pr(X \leq x) \cdot Pr(Y \leq y) \Rightarrow F(x,y) = F_X(x) \cdot F_Y(y)$.

    \subparagraph*{Conditional Distributions: }
        $X, Y$ discrete r.v. with p.f. $f_X(x), f_Y(y)$ and joint p.f. $f(x,y)$. Then:
        \begin{itemize}
            \item[]
            \item[] $Pr(X = x | Y = y) = \frac{Pr(X = x, Y = y)}{Pr(Y = y)} = \frac{f(x,y)}{f_Y(y)}$.
            \item[]
        \end{itemize}
        This is a new distribution and the p.f. is (p.f. of $X|Y$):
        \begin{itemize}
            \item[] $g_X(x|y) = \begin{cases} \frac{f(x,y)}{f_Y(y)} & \forall x,y: f_Y(y) > 0 \\ 0 & \text{o.w.}\end{cases}$
        \end{itemize}
        Note that 
        \begin{itemize}
            \item[]$\sum_x y_X(x|y) = \sum_x \frac{f(x,y)}{f_Y(y)} $
             $= \frac{1}{f_Y(y)} \cdot \sum_xf(x,y) = \frac{f_Y(y)}{f_Y(y)} = 1$. 
            \item[]
        \end{itemize}
    We can also define the conditional distribution of $Y$ given $X=x$ by:
        \begin{itemize}
            \item[] $g_Y(y|x) = \begin{cases} \frac{f(x,y)}{f_X(x)} & \forall x,y: f_X(x) > 0 \\ 0 & \text{o.w.} \end{cases}$
        \end{itemize}
    In the continuous case $X, Y$ with joint p.d.f. $f(x,y)$ and marginal p.d.f.'s $f_X(x)$ and $f_Y(y)$: 
\begin{itemize}
\item[] $g_X(x|y) = \begin{cases} \frac{f(x,y)}{f_Y(y)} & f_Y(y) > 0 \\ 0 & \text{o.w.} \end{cases} $
\item[] $g_Y(y|x) = \begin{cases} \frac{f(x,y)}{f_X(x)} & f_X(x) > 0 \\ 0 & \text{o.w.} \end{cases}$.
\end{itemize}
 Again note that 
 \begin{itemize}
    \item[]$\int\limits_{-\infty}^{\infty}g_X(x|y)dx = \int\limits_{-\infty}^{\infty} \frac{f(x,y)}{f_Y(y)}dx = \frac{1}{f_Y(y)} \int\limits_{-\infty}^{\infty}f(x,y)dx $
    \item[] $= \frac{1}{f_Y(y)} \cdot f_Y(y) = 1$.
 \end{itemize}

    \subparagraph*{Other Stuff:}
    \begin{itemize}
        \item[] $\sum\limits_{i=1}^{n}i = \frac{n(n+1)}{2},\sum\limits_{i=1}^{n}i^2 = \frac{n(n+1)(2n+1)}{6}$
        \item[] $\sum\limits_{i=0}^{n}c^i = \frac{c^{n+1}-1}{c-1}, c \neq 1; \sum\limits_{i=0}^{\infty}c^i = \frac{1}{1-c}$
        \item[] $\frac{d}{dx}(a^x) = \ln a$
        \item[] $(fg)' = f'g + fg'$
        \item[] $\left(\frac{f}{g}\right)' = \frac{f'g-fg'}{g^2}$
        \item[] $\frac{d}{dx}(f(g(x)))=f'(g(x))g'(x)$
        \item[] $\frac{d}{dx}(e^{g(x)})= g'(x)e^{g(x)}$
        \item[] $\frac{d}{dx}(\ln g(x)) = \frac{g'(x)}{g(x)}$
        \item[] $\int \frac{1}{x}dx = \ln |x| + c$, $\int \frac{1}{ax+b}dx=\frac{1}{a}\ln |ax+b|+c$
        \item[] $\int e^u du = e^u + c$, $\int a^udu = \frac{a^u}{\ln a} + c$
        \item[] $\int_a^bf(g(x))g'(x)dx \Rightarrow u=g(x) \Rightarrow \int_{g(a)}^{g(b)}f(u)du$
        \item[] $\int udv = uv - \int vdu$
    \end{itemize}
    \end{multicols}

\end{document}